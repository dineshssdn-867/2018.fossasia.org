{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dineshssdn-867/2018.fossasia.org/blob/master/code/Anomaly_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rlx1qkQfWzRg"
      },
      "source": [
        "# Import Libraries and config files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6cWDAwOgob_",
        "outputId": "dc3d6cd0-a716-41e1-b9d6-600f4185ca4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy==1.1.0\n",
            "  Downloading scipy-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (31.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 31.2 MB 89.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.1.0) (1.21.6)\n",
            "Installing collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pymc3 3.11.4 requires scipy>=1.2.0, but you have scipy 1.1.0 which is incompatible.\n",
            "plotnine 0.6.0 requires scipy>=1.2.0, but you have scipy 1.1.0 which is incompatible.\n",
            "jax 0.3.4 requires scipy>=1.2.1, but you have scipy 1.1.0 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed scipy-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install scipy==1.1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uY4XWwKFWvDI"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import scipy.io\n",
        "import os\n",
        "import gc\n",
        "import sklearn.preprocessing\n",
        "import matplotlib\n",
        "import tensorflow\n",
        "import keras.optimizers\n",
        "import warnings\n",
        "import scipy.io as sio\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter \n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.regularizers import l2\n",
        "from keras.models import Model\n",
        "from keras.layers.core import Dense, Dropout, Flatten\n",
        "from keras.layers.convolutional import Conv3D, MaxPooling3D, ZeroPadding3D, AveragePooling3D \n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, Input\n",
        "from tensorflow.keras import layers\n",
        "from scipy.misc import imresize\n",
        "from keras.models import model_from_json\n",
        "from datetime import datetime\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.utils.layer_utils import get_source_inputs\n",
        "from keras.utils import layer_utils\n",
        "import matplotlib.pyplot as plt\n",
        "matplotlib.use('Agg')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Input\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Conv3D\n",
        "from keras.layers import MaxPooling3D\n",
        "from keras.layers import AveragePooling3D\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Lambda\n",
        "from keras.layers import GlobalAveragePooling3D"
      ],
      "metadata": {
        "id": "neu6BXJwA6WG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2CQgeb90Xijg"
      },
      "outputs": [],
      "source": [
        "C3D_MEAN_PATH = 'https://github.com/adamcasson/c3d/releases/download/v0.1/c3d_mean.npy'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# path to pretrained models with no top (no classification layer)\n",
        "WEIGHTS_PATH_NO_TOP = {\n",
        "    'rgb_kinetics_only' : 'https://github.com/dlpbc/keras-kinetics-i3d/releases/download/v0.2/rgb_inception_i3d_kinetics_only_tf_dim_ordering_tf_kernels_no_top.h5',\n",
        "    'flow_kinetics_only' : 'https://github.com/dlpbc/keras-kinetics-i3d/releases/download/v0.2/flow_inception_i3d_kinetics_only_tf_dim_ordering_tf_kernels_no_top.h5',\n",
        "    'rgb_imagenet_and_kinetics' : 'https://github.com/dlpbc/keras-kinetics-i3d/releases/download/v0.2/rgb_inception_i3d_imagenet_and_kinetics_tf_dim_ordering_tf_kernels_no_top.h5',\n",
        "    'flow_imagenet_and_kinetics' : 'https://github.com/dlpbc/keras-kinetics-i3d/releases/download/v0.2/flow_inception_i3d_imagenet_and_kinetics_tf_dim_ordering_tf_kernels_no_top.h5'\n",
        "}\n",
        "\n",
        "WEIGHTS_PATH = {\n",
        "    'rgb_kinetics_only' : 'https://github.com/dlpbc/keras-kinetics-i3d/releases/download/v0.2/rgb_inception_i3d_kinetics_only_tf_dim_ordering_tf_kernels.h5',\n",
        "    'flow_kinetics_only' : 'https://github.com/dlpbc/keras-kinetics-i3d/releases/download/v0.2/flow_inception_i3d_kinetics_only_tf_dim_ordering_tf_kernels.h5',\n",
        "    'rgb_imagenet_and_kinetics' : 'https://github.com/dlpbc/keras-kinetics-i3d/releases/download/v0.2/rgb_inception_i3d_imagenet_and_kinetics_tf_dim_ordering_tf_kernels.h5',\n",
        "    'flow_imagenet_and_kinetics' : 'https://github.com/dlpbc/keras-kinetics-i3d/releases/download/v0.2/flow_inception_i3d_imagenet_and_kinetics_tf_dim_ordering_tf_kernels.h5'\n",
        "}"
      ],
      "metadata": {
        "id": "MX31zByjwfva"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cy783NF3-YLG",
        "outputId": "3ac99903-c9f4-416c-ccb8-9ac2b160b4fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOTjHUZQY_Ze"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TfmFHfAlYToJ"
      },
      "outputs": [],
      "source": [
        "c3d_model_weights = '/content/drive/MyDrive/Anomaly Recognition/sports1M_weights_tf.h5'\n",
        "abnormal_videos_path=\"/content/drive/MyDrive/Anomaly Recognition/Anomaly Videos\"\n",
        "normal_videos_path=\"/content/drive/MyDrive/Anomaly Recognition/Normal Videos/Training-Normal-Videos-Part-1\"\n",
        "raw_normal_train_features_c3d=\"/content/drive/MyDrive/Anomaly Recognition/Raw Normal videos features C3d\"\n",
        "raw_abnormal_train_features_c3d=\"/content/drive/MyDrive/Anomaly Recognition/Raw Anomaly videos features C3d\"\n",
        "processed_normal_train_features_c3d=\"/content/drive/MyDrive/Anomaly Recognition/Pre-processed Normal Video features C3d\"\n",
        "processed_abnormal_train_features_c3d=\"/content/drive/MyDrive/Anomaly Recognition/Pre-processed Anomaly video features C3d\"\n",
        "raw_normal_train_features_i3d=\"/content/drive/MyDrive/Anomaly Recognition/Raw Normal videos features i3d\"\n",
        "raw_abnormal_train_features_i3d=\"/content/drive/MyDrive/Anomaly Recognition/Raw Anomaly videos features i3d\"\n",
        "processed_normal_train_features_i3d=\"/content/drive/MyDrive/Anomaly Recognition/Pre-processed Normal Video features i3d\"\n",
        "processed_abnormal_train_features_i3d=\"/content/drive/MyDrive/Anomaly Recognition/Pre-processed Anomaly video features i3d\"\n",
        "output_dir = \"/content/drive/MyDrive/Anomaly Recognition/Trained_models/\"\n",
        "output_folder_c3d = '/content/drive/MyDrive/Anomaly Recognition/output_videos'\n",
        "output_folder_i3d = '/content/drive/MyDrive/Anomaly Recognition/output_videos_i3d'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WEIGHTS_NAME = ['rgb_kinetics_only', 'flow_kinetics_only', 'rgb_imagenet_and_kinetics', 'flow_imagenet_and_kinetics']"
      ],
      "metadata": {
        "id": "pDvI3lpPAPZ9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3_Q2mN44Yb6T"
      },
      "outputs": [],
      "source": [
        "frame_height = 240\n",
        "frame_width = 320\n",
        "channels = 3\n",
        "\n",
        "c3d_frame_count = 16\n",
        "i3d_frame_count = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QP-zhH9HGhn4"
      },
      "outputs": [],
      "source": [
        "features_per_bag = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0d-SfyjooKD"
      },
      "source": [
        "## Chunks work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EQrbeOpDof_R"
      },
      "outputs": [],
      "source": [
        "def divide_chunks(l, n):\n",
        "      \"\"\"Apply divide_chunks to an array, getting chunks of\n",
        "    of specified size\n",
        "    \"\"\"\n",
        "    for i in range(0, len(l), n): \n",
        "        yield l[i:i + n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUHtf1rrY83b"
      },
      "source": [
        "## Array Util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qoMmbjroYnQ5"
      },
      "outputs": [],
      "source": [
        "def sliding_window(arr, size, stride):\n",
        "    \"\"\"Apply sliding window to an array, getting chunks of\n",
        "    of specified size using the specified stride\n",
        "    :param arr: Array to be divided\n",
        "    :param size: Size of the chunks\n",
        "    :param stride: Number of frames to skip for the next chunk\n",
        "    :returns: Tensor with the resulting chunks\n",
        "    :rtype: np.ndarray\n",
        "    \"\"\"\n",
        "    num_chunks = int((len(arr) - size) / stride) + 2\n",
        "    result = []\n",
        "    for i in range(0,  num_chunks * stride, stride):\n",
        "        if len(arr[i:i + size]) > 0:\n",
        "            result.append(arr[i:i + size])\n",
        "    return np.array(result)\n",
        "\n",
        "\n",
        "def interpolate(features, features_per_bag):\n",
        "    \"\"\"Transform a bag with an arbitrary number of features into a bag\n",
        "    with a fixed amount, using interpolation of consecutive features\n",
        "    :param features: Bag of features to pad\n",
        "    :param features_per_bag: Number of features to obtain\n",
        "    :returns: Interpolated features\n",
        "    :rtype: np.ndarray\n",
        "    \"\"\"\n",
        "    feature_size = np.array(features).shape[1]\n",
        "    interpolated_features = np.zeros((features_per_bag, feature_size))\n",
        "    interpolation_indices = np.round(np.linspace(0, len(features) - 1, num=features_per_bag + 1))\n",
        "    count = 0\n",
        "    for index in range(0, len(interpolation_indices)-1):\n",
        "        start = int(interpolation_indices[index])\n",
        "        end = int(interpolation_indices[index + 1])\n",
        "\n",
        "        assert end >= start\n",
        "\n",
        "        if start == end:\n",
        "            temp_vect = features[start, :]\n",
        "        else:\n",
        "            temp_vect = np.mean(features[start:end+1, :], axis=0)\n",
        "\n",
        "        temp_vect = temp_vect / np.linalg.norm(temp_vect)\n",
        "\n",
        "        if np.linalg.norm(temp_vect) == 0:\n",
        "            print(\"Error\")\n",
        "\n",
        "        interpolated_features[count,:]=temp_vect\n",
        "        count = count + 1\n",
        "    del interpolation_indices\n",
        "    gc.collect(generation=2)\n",
        "    return np.array(interpolated_features)\n",
        "\n",
        "\n",
        "def extrapolate(outputs, num_frames):\n",
        "    \"\"\"Expand output to match the video length\n",
        "    :param outputs: Array of predicted outputs\n",
        "    :param num_frames: Expected size of the output array\n",
        "    :returns: Array of output size\n",
        "    :rtype: np.ndarray\n",
        "    \"\"\"\n",
        "\n",
        "    extrapolated_outputs = []\n",
        "    extrapolation_indices = np.round(np.linspace(0, len(outputs) - 1, num=num_frames))\n",
        "    for index in extrapolation_indices:\n",
        "        extrapolated_outputs.append(outputs[int(index)])\n",
        "    del extrapolation_indices\n",
        "    gc.collect(generation=2)\n",
        "    return np.array(extrapolated_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4slLmqd_ZFej"
      },
      "source": [
        "# Video Util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "abpVcpA1Y6aT"
      },
      "outputs": [],
      "source": [
        "def get_video_clips(video_path, frame_count):\n",
        "    \"\"\"Divides the input video into non-overlapping clips\n",
        "    :param video_path: Path to the video\n",
        "    :returns: Array with the fragments of video\n",
        "    :rtype: np.ndarray\n",
        "    \"\"\"\n",
        "    frames = get_video_frames(video_path)\n",
        "    clips = sliding_window(frames, frame_count, frame_count)\n",
        "    return clips, len(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "T-zPMiagG26K"
      },
      "outputs": [],
      "source": [
        "def get_video_frames(video_path):\n",
        "    \"\"\"Reads the video given a file path\n",
        "    :param video_path: Path to the video\n",
        "    :returns: Video as an array of frames\n",
        "    :rtype: np.ndarray\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    while (cap.isOpened()):\n",
        "        ret, frame = cap.read()\n",
        "        if ret == True:\n",
        "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "            del frame\n",
        "        else:\n",
        "            break\n",
        "    cap.release()\n",
        "    del cap\n",
        "    gc.collect(generation=2)\n",
        "    return frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91Whhz7wYjv4"
      },
      "source": [
        "# Video pre-processing C3d\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "POQ5Ob3HYme8"
      },
      "outputs": [],
      "source": [
        "def preprocess_input_c3d(video):\n",
        "    \"\"\"Preprocess video input to make it suitable for feature extraction.\n",
        "    The video is resized, cropped, resampled and training mean is substracted\n",
        "    to make it suitable for the network\n",
        "    :param video: Video to be processed\n",
        "    :returns: Preprocessed video\n",
        "    :rtype: np.ndarray\n",
        "    \"\"\"\n",
        "\n",
        "    intervals = np.ceil(np.linspace(0, video.shape[0] - 1, 16)).astype(int)\n",
        "    frames = video[intervals]\n",
        "\n",
        "    # Reshape to 128x171\n",
        "    reshape_frames = np.zeros((frames.shape[0], 128, 171, frames.shape[3]))\n",
        "    for i, img in enumerate(frames):\n",
        "        img = imresize(img, (128, 171), 'bicubic')\n",
        "        reshape_frames[i, :, :, :] = img\n",
        "        del img\n",
        "\n",
        "    mean_path = get_file('c3d_mean.npy',\n",
        "                         C3D_MEAN_PATH,\n",
        "                         cache_subdir='models',\n",
        "                         md5_hash='08a07d9761e76097985124d9e8b2fe34')\n",
        "\n",
        "    mean = np.load(mean_path)\n",
        "    reshape_frames -= mean\n",
        "    # Crop to 112x112\n",
        "    reshape_frames = reshape_frames[:, 8:120, 30:142, :]\n",
        "    # Add extra dimension for samples\n",
        "    reshape_frames = np.expand_dims(reshape_frames, axis=0)\n",
        "    del intervals\n",
        "    del frames\n",
        "    gc.collect(generation=2)\n",
        "    return reshape_frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk7F2AMCCdSA"
      },
      "source": [
        "# Video pre-processing I3d\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7FeVAGqOCdSB"
      },
      "outputs": [],
      "source": [
        "def preprocess_input_i3d(video):\n",
        "    \"\"\"Preprocess video input to make it suitable for feature extraction.\n",
        "    The video is resized, cropped, resampled and training mean is substracted\n",
        "    to make it suitable for the network\n",
        "    :param video: Video to be processed\n",
        "    :returns: Preprocessed video\n",
        "    :rtype: np.ndarray\n",
        "    \"\"\"\n",
        "    \n",
        "    intervals = np.ceil(np.linspace(0, video.shape[0] - 1, 16)).astype(int)\n",
        "    frames = video[intervals]\n",
        "\n",
        "    # Reshape to 224x224\n",
        "    reshape_frames = np.zeros((frames.shape[0], 224, 224, frames.shape[3]))\n",
        "    for i, img in enumerate(frames):\n",
        "        img = imresize(img, (224, 224), 'bicubic')\n",
        "        reshape_frames[i, :, :, :] = img\n",
        "        del img\n",
        "\n",
        "    reshape_frames = np.expand_dims(reshape_frames, axis=0)\n",
        "    del intervals\n",
        "    del frames\n",
        "    gc.collect(generation=2)\n",
        "    return reshape_frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0zyF-v_CZCg"
      },
      "source": [
        "# Segment Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VCvamAy-hmlf"
      },
      "outputs": [],
      "source": [
        "def transform_into_segments(features, n_segments=32):\n",
        "  \"\"\"This will convert the vector \n",
        "  into 32 segments of feature vectors\"\"\"\n",
        "    if features.shape[0] < n_segments:\n",
        "        raise RuntimeError(\n",
        "            \"Number of prev segments lesser than expected output size\"\n",
        "        )\n",
        "\n",
        "    cuts = np.linspace(0, features.shape[0], n_segments,\n",
        "                       dtype=int, endpoint=False)\n",
        "\n",
        "    new_feats = []\n",
        "    for i, j in zip(cuts[:-1], cuts[1:]):\n",
        "        new_feats.append(np.mean(features[i:j,:], axis=0))\n",
        "\n",
        "    new_feats.append(np.mean(features[cuts[-1]:,:], axis=0))\n",
        "\n",
        "    new_feats = np.array(new_feats)\n",
        "    new_feats = sklearn.preprocessing.normalize(new_feats, axis=1)\n",
        "    del cuts\n",
        "    gc.collect(generation=2)\n",
        "    return new_feats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjY1bDQCbxMr"
      },
      "source": [
        "# C3d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "L-fcmxrtbeQX"
      },
      "outputs": [],
      "source": [
        "def C3D(weights='sports1M'):\n",
        "    \"\"\"Creation of the full C3D architecture\n",
        "    :param weights: Weights to be loaded into the network. If None,\n",
        "    the network is randomly initialized.\n",
        "    :returns: Network model\n",
        "    :rtype: keras.model\n",
        "    \"\"\"\n",
        "\n",
        "    if weights not in {'sports1M', None}:\n",
        "        raise ValueError('weights should be either be sports1M or None')\n",
        "\n",
        "    if K.image_data_format() == 'channels_last':\n",
        "        shape = (16, 112, 112, 3)\n",
        "    else:\n",
        "        shape = (3, 16, 112, 112)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(\n",
        "        Conv3D(64,\n",
        "               3,\n",
        "               activation='relu',\n",
        "               padding='same',\n",
        "               name='conv1',\n",
        "               input_shape=shape))\n",
        "    model.add(\n",
        "        MaxPooling3D(pool_size=(1, 2, 2),\n",
        "                     strides=(1, 2, 2),\n",
        "                     padding='same',\n",
        "                     name='pool1'))\n",
        "\n",
        "    model.add(Conv3D(128, 3, activation='relu', padding='same', name='conv2'))\n",
        "    model.add(\n",
        "        MaxPooling3D(pool_size=(2, 2, 2),\n",
        "                     strides=(2, 2, 2),\n",
        "                     padding='valid',\n",
        "                     name='pool2'))\n",
        "\n",
        "    model.add(Conv3D(256, 3, activation='relu', padding='same', name='conv3a'))\n",
        "    model.add(Conv3D(256, 3, activation='relu', padding='same', name='conv3b'))\n",
        "    model.add(\n",
        "        MaxPooling3D(pool_size=(2, 2, 2),\n",
        "                     strides=(2, 2, 2),\n",
        "                     padding='valid',\n",
        "                     name='pool3'))\n",
        "\n",
        "    model.add(Conv3D(512, 3, activation='relu', padding='same', name='conv4a'))\n",
        "    model.add(Conv3D(512, 3, activation='relu', padding='same', name='conv4b'))\n",
        "    model.add(\n",
        "        MaxPooling3D(pool_size=(2, 2, 2),\n",
        "                     strides=(2, 2, 2),\n",
        "                     padding='valid',\n",
        "                     name='pool4'))\n",
        "\n",
        "    model.add(Conv3D(512, 3, activation='relu', padding='same', name='conv5a'))\n",
        "    model.add(Conv3D(512, 3, activation='relu', padding='same', name='conv5b'))\n",
        "    model.add(ZeroPadding3D(padding=(0, 1, 1)))\n",
        "    model.add(\n",
        "        MaxPooling3D(pool_size=(2, 2, 2),\n",
        "                     strides=(2, 2, 2),\n",
        "                     padding='valid',\n",
        "                     name='pool5'))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(4096, activation='relu', name='fc6'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(4096, activation='relu', name='fc7'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(487, activation='softmax', name='fc8'))\n",
        "\n",
        "    if weights == 'sports1M':\n",
        "        model.load_weights(c3d_model_weights)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mMPYnJ3mb3-x"
      },
      "outputs": [],
      "source": [
        "def c3d_feature_extractor():\n",
        "    \"\"\"Creation of the feature extraction architecture. This network is\n",
        "    formed by a subset of the original C3D architecture (from the\n",
        "    beginning to fc6 layer)\n",
        "    :returns: Feature extraction model\n",
        "    :rtype: keras.model\n",
        "    \"\"\"\n",
        "    model = C3D()\n",
        "    layer_name = 'fc6'\n",
        "    feature_extractor_model = Model(inputs=model.input,\n",
        "                                    outputs=model.get_layer(layer_name).output)\n",
        "    return feature_extractor_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rQmNkfJIcMZT"
      },
      "outputs": [],
      "source": [
        "feature_extractor = c3d_feature_extractor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjDu9u66PtQ3"
      },
      "source": [
        "# I3d"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _obtain_input_shape(input_shape,\n",
        "                        default_frame_size,\n",
        "                        min_frame_size,\n",
        "                        default_num_frames,\n",
        "                        min_num_frames,\n",
        "                        data_format,\n",
        "                        require_flatten,\n",
        "                        weights=None):\n",
        "    \"\"\"Internal utility to compute/validate the model's input shape.\n",
        "    (Adapted from `keras/applications/imagenet_utils.py`)\n",
        "    # Arguments\n",
        "        input_shape: either None (will return the default network input shape),\n",
        "            or a user-provided shape to be validated.\n",
        "        default_frame_size: default input frames(images) width/height for the model.\n",
        "        min_frame_size: minimum input frames(images) width/height accepted by the model.\n",
        "        default_num_frames: default input number of frames(images) for the model.\n",
        "        min_num_frames: minimum input number of frames accepted by the model.\n",
        "        data_format: image data format to use.\n",
        "        require_flatten: whether the model is expected to\n",
        "            be linked to a classifier via a Flatten layer.\n",
        "        weights: one of `None` (random initialization)\n",
        "            or 'kinetics_only' (pre-training on Kinetics dataset).\n",
        "            or 'imagenet_and_kinetics' (pre-training on ImageNet and Kinetics datasets).\n",
        "            If weights='kinetics_only' or weights=='imagenet_and_kinetics' then\n",
        "            input channels must be equal to 3.\n",
        "    # Returns\n",
        "        An integer shape tuple (may include None entries).\n",
        "    # Raises\n",
        "        ValueError: in case of invalid argument values.\n",
        "    \"\"\"\n",
        "    if weights != 'kinetics_only' and weights != 'imagenet_and_kinetics' and input_shape and len(input_shape) == 4:\n",
        "        if data_format == 'channels_first':\n",
        "            if input_shape[0] not in {1, 3}:\n",
        "                warnings.warn(\n",
        "                    'This model usually expects 1 or 3 input channels. '\n",
        "                    'However, it was passed an input_shape with ' +\n",
        "                    str(input_shape[0]) + ' input channels.')\n",
        "            default_shape = (input_shape[0], default_num_frames, default_frame_size, default_frame_size)\n",
        "        else:\n",
        "            if input_shape[-1] not in {1, 3}:\n",
        "                warnings.warn(\n",
        "                    'This model usually expects 1 or 3 input channels. '\n",
        "                    'However, it was passed an input_shape with ' +\n",
        "                    str(input_shape[-1]) + ' input channels.')\n",
        "            default_shape = (default_num_frames, default_frame_size, default_frame_size, input_shape[-1])\n",
        "    else:\n",
        "        if data_format == 'channels_first':\n",
        "            default_shape = (3, default_num_frames, default_frame_size, default_frame_size)\n",
        "        else:\n",
        "            default_shape = (default_num_frames, default_frame_size, default_frame_size, 3)\n",
        "    if (weights == 'kinetics_only' or weights == 'imagenet_and_kinetics') and require_flatten:\n",
        "        if input_shape is not None:\n",
        "            if input_shape != default_shape:\n",
        "                raise ValueError('When setting`include_top=True` '\n",
        "                                 'and loading `imagenet` weights, '\n",
        "                                 '`input_shape` should be ' +\n",
        "                                 str(default_shape) + '.')\n",
        "        return default_shape\n",
        "\n",
        "    if input_shape:\n",
        "        if data_format == 'channels_first':\n",
        "            if input_shape is not None:\n",
        "                if len(input_shape) != 4:\n",
        "                    raise ValueError(\n",
        "                        '`input_shape` must be a tuple of four integers.')\n",
        "                if input_shape[0] != 3 and (weights == 'kinetics_only' or weights == 'imagenet_and_kinetics'):\n",
        "                    raise ValueError('The input must have 3 channels; got '\n",
        "                                     '`input_shape=' + str(input_shape) + '`')\n",
        "\n",
        "                if input_shape[1] is not None and input_shape[1] < min_num_frames:\n",
        "                    raise ValueError('Input number of frames must be at least ' +\n",
        "                                     str(min_num_frames) + '; got '\n",
        "                                     '`input_shape=' + str(input_shape) + '`')\n",
        "\n",
        "                if ((input_shape[2] is not None and input_shape[2] < min_frame_size) or\n",
        "                   (input_shape[3] is not None and input_shape[3] < min_frame_size)):\n",
        "                    raise ValueError('Input size must be at least ' +\n",
        "                                     str(min_frame_size) + 'x' + str(min_frame_size) + '; got '\n",
        "                                     '`input_shape=' + str(input_shape) + '`')\n",
        "        else:\n",
        "            if input_shape is not None:\n",
        "                if len(input_shape) != 4:\n",
        "                    raise ValueError(\n",
        "                        '`input_shape` must be a tuple of four integers.')\n",
        "                if input_shape[-1] != 3 and (weights == 'kinetics_only' or weights == 'imagenet_and_kinetics'):\n",
        "                    raise ValueError('The input must have 3 channels; got '\n",
        "                                     '`input_shape=' + str(input_shape) + '`')\n",
        "\n",
        "                if input_shape[0] is not None and input_shape[0] < min_num_frames:\n",
        "                    raise ValueError('Input number of frames must be at least ' +\n",
        "                                     str(min_num_frames) + '; got '\n",
        "                                     '`input_shape=' + str(input_shape) + '`')\n",
        "\n",
        "                if ((input_shape[1] is not None and input_shape[1] < min_frame_size) or\n",
        "                   (input_shape[2] is not None and input_shape[2] < min_frame_size)):\n",
        "                    raise ValueError('Input size must be at least ' +\n",
        "                                     str(min_frame_size) + 'x' + str(min_frame_size) + '; got '\n",
        "                                     '`input_shape=' + str(input_shape) + '`')\n",
        "    else:\n",
        "        if require_flatten:\n",
        "            input_shape = default_shape\n",
        "        else:\n",
        "            if data_format == 'channels_first':\n",
        "                input_shape = (3, None, None, None)\n",
        "            else:\n",
        "                input_shape = (None, None, None, 3)\n",
        "    if require_flatten:\n",
        "        if None in input_shape:\n",
        "            raise ValueError('If `include_top` is True, '\n",
        "                             'you should specify a static `input_shape`. '\n",
        "                             'Got `input_shape=' + str(input_shape) + '`')\n",
        "    return input_shape\n"
      ],
      "metadata": {
        "id": "XpbF56GrwcP-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZUeJqVeg36E8"
      },
      "outputs": [],
      "source": [
        "def conv3d_bn(x,\n",
        "              filters,\n",
        "              num_frames,\n",
        "              num_row,\n",
        "              num_col,\n",
        "              padding='same',\n",
        "              strides=(1, 1, 1),\n",
        "              use_bias = False,\n",
        "              use_activation_fn = True,\n",
        "              use_bn = True,\n",
        "              name=None):\n",
        "    \"\"\"Utility function to apply conv3d + BN.\n",
        "    # Arguments\n",
        "        x: input tensor.\n",
        "        filters: filters in `Conv3D`.\n",
        "        num_frames: frames (time depth) of the convolution kernel.\n",
        "        num_row: height of the convolution kernel.\n",
        "        num_col: width of the convolution kernel.\n",
        "        padding: padding mode in `Conv3D`.\n",
        "        strides: strides in `Conv3D`.\n",
        "        use_bias: use bias or not  \n",
        "        use_activation_fn: use an activation function or not.\n",
        "        use_bn: use batch normalization or not.\n",
        "        name: name of the ops; will become `name + '_conv'`\n",
        "            for the convolution and `name + '_bn'` for the\n",
        "            batch norm layer.\n",
        "    # Returns\n",
        "        Output tensor after applying `Conv3D` and `BatchNormalization`.\n",
        "    \"\"\"\n",
        "    if name is not None:\n",
        "        bn_name = name + '_bn'\n",
        "        conv_name = name + '_conv'\n",
        "    else:\n",
        "        bn_name = None\n",
        "        conv_name = None\n",
        "\n",
        "    x = Conv3D(\n",
        "        filters, (num_frames, num_row, num_col),\n",
        "        strides=strides,\n",
        "        padding=padding,\n",
        "        use_bias=use_bias,\n",
        "        name=conv_name)(x)\n",
        "\n",
        "    if use_bn:\n",
        "        if K.image_data_format() == 'channels_first':\n",
        "            bn_axis = 1\n",
        "        else:\n",
        "            bn_axis = 4\n",
        "        x = BatchNormalization(axis=bn_axis, scale=False, name=bn_name)(x)\n",
        "\n",
        "    if use_activation_fn:\n",
        "        x = Activation('relu', name=name)(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Inception_Inflated3d(include_top=True,\n",
        "                weights=None,\n",
        "                input_tensor=None,\n",
        "                input_shape=None,\n",
        "                dropout_prob=0.0,\n",
        "                endpoint_logit=True,\n",
        "                classes=400):\n",
        "    \"\"\"Instantiates the Inflated 3D Inception v1 architecture.\n",
        "    Optionally loads weights pre-trained\n",
        "    on Kinetics. Note that when using TensorFlow,\n",
        "    for best performance you should set\n",
        "    `image_data_format='channels_last'` in your Keras config\n",
        "    at ~/.keras/keras.json.\n",
        "    The model and the weights are compatible with both\n",
        "    TensorFlow and Theano. The data format\n",
        "    convention used by the model is the one\n",
        "    specified in your Keras config file.\n",
        "    Note that the default input frame(image) size for this model is 224x224.\n",
        "    # Arguments\n",
        "        include_top: whether to include the the classification \n",
        "            layer at the top of the network.\n",
        "        weights: one of `None` (random initialization)\n",
        "            or 'kinetics_only' (pre-training on Kinetics dataset only).\n",
        "            or 'imagenet_and_kinetics' (pre-training on ImageNet and Kinetics datasets).\n",
        "        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
        "            to use as image input for the model.\n",
        "        input_shape: optional shape tuple, only to be specified\n",
        "            if `include_top` is False (otherwise the input shape\n",
        "            has to be `(NUM_FRAMES, 224, 224, 3)` (with `channels_last` data format)\n",
        "            or `(NUM_FRAMES, 3, 224, 224)` (with `channels_first` data format).\n",
        "            It should have exactly 3 inputs channels.\n",
        "            NUM_FRAMES should be no smaller than 8. The authors used 64\n",
        "            frames per example for training and testing on kinetics dataset\n",
        "            Also, Width and height should be no smaller than 32.\n",
        "            E.g. `(64, 150, 150, 3)` would be one valid value.\n",
        "        dropout_prob: optional, dropout probability applied in dropout layer\n",
        "            after global average pooling layer. \n",
        "            0.0 means no dropout is applied, 1.0 means dropout is applied to all features.\n",
        "            Note: Since Dropout is applied just before the classification\n",
        "            layer, it is only useful when `include_top` is set to True.\n",
        "        endpoint_logit: (boolean) optional. If True, the model's forward pass\n",
        "            will end at producing logits. Otherwise, softmax is applied after producing\n",
        "            the logits to produce the class probabilities prediction. Setting this parameter \n",
        "            to True is particularly useful when you want to combine results of rgb model\n",
        "            and optical flow model.\n",
        "            - `True` end model forward pass at logit output\n",
        "            - `False` go further after logit to produce softmax predictions\n",
        "            Note: This parameter is only useful when `include_top` is set to True.\n",
        "        classes: optional number of classes to classify images\n",
        "            into, only to be specified if `include_top` is True, and\n",
        "            if no `weights` argument is specified.\n",
        "    # Returns\n",
        "        A Keras model instance.\n",
        "    # Raises\n",
        "        ValueError: in case of invalid argument for `weights`,\n",
        "            or invalid input shape.\n",
        "    \"\"\"\n",
        "    if not (weights in WEIGHTS_NAME or weights is None or os.path.exists(weights)):\n",
        "        raise ValueError('The `weights` argument should be either '\n",
        "                         '`None` (random initialization) or %s' % \n",
        "                         str(WEIGHTS_NAME) + ' ' \n",
        "                         'or a valid path to a file containing `weights` values')\n",
        "\n",
        "    if weights in WEIGHTS_NAME and include_top and classes != 400:\n",
        "        raise ValueError('If using `weights` as one of these %s, with `include_top`'\n",
        "                         ' as true, `classes` should be 400' % str(WEIGHTS_NAME))\n",
        "\n",
        "    # Determine proper input shape\n",
        "    input_shape = _obtain_input_shape(\n",
        "        input_shape,\n",
        "        default_frame_size=224, \n",
        "        min_frame_size=32, \n",
        "        default_num_frames=64,\n",
        "        min_num_frames=8,\n",
        "        data_format=K.image_data_format(),\n",
        "        require_flatten=include_top,\n",
        "        weights=weights)\n",
        "\n",
        "    if input_tensor is None:\n",
        "        img_input = Input(shape=input_shape)\n",
        "    else:\n",
        "        if not K.is_keras_tensor(input_tensor):\n",
        "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
        "        else:\n",
        "            img_input = input_tensor\n",
        "\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        channel_axis = 1\n",
        "    else:\n",
        "        channel_axis = 4\n",
        "\n",
        "    # Downsampling via convolution (spatial and temporal)\n",
        "    x = conv3d_bn(img_input, 64, 7, 7, 7, strides=(2, 2, 2), padding='same', name='Conv3d_1a_7x7')\n",
        "\n",
        "    # Downsampling (spatial only)\n",
        "    x = MaxPooling3D((1, 3, 3), strides=(1, 2, 2), padding='same', name='MaxPool2d_2a_3x3')(x)\n",
        "    x = conv3d_bn(x, 64, 1, 1, 1, strides=(1, 1, 1), padding='same', name='Conv3d_2b_1x1')\n",
        "    x = conv3d_bn(x, 192, 3, 3, 3, strides=(1, 1, 1), padding='same', name='Conv3d_2c_3x3')\n",
        "\n",
        "    # Downsampling (spatial only)\n",
        "    x = MaxPooling3D((1, 3, 3), strides=(1, 2, 2), padding='same', name='MaxPool2d_3a_3x3')(x)\n",
        "\n",
        "    # Mixed 3b\n",
        "    branch_0 = conv3d_bn(x, 64, 1, 1, 1, padding='same', name='Conv3d_3b_0a_1x1')\n",
        "\n",
        "    branch_1 = conv3d_bn(x, 96, 1, 1, 1, padding='same', name='Conv3d_3b_1a_1x1')\n",
        "    branch_1 = conv3d_bn(branch_1, 128, 3, 3, 3, padding='same', name='Conv3d_3b_1b_3x3')\n",
        "\n",
        "    branch_2 = conv3d_bn(x, 16, 1, 1, 1, padding='same', name='Conv3d_3b_2a_1x1')\n",
        "    branch_2 = conv3d_bn(branch_2, 32, 3, 3, 3, padding='same', name='Conv3d_3b_2b_3x3')\n",
        "\n",
        "    branch_3 = MaxPooling3D((3, 3, 3), strides=(1, 1, 1), padding='same', name='MaxPool2d_3b_3a_3x3')(x)\n",
        "    branch_3 = conv3d_bn(branch_3, 32, 1, 1, 1, padding='same', name='Conv3d_3b_3b_1x1')\n",
        "\n",
        "    x = layers.concatenate(\n",
        "        [branch_0, branch_1, branch_2, branch_3],\n",
        "        axis=channel_axis,\n",
        "        name='Mixed_3b')\n",
        "\n",
        "    # Mixed 3c\n",
        "    branch_0 = conv3d_bn(x, 128, 1, 1, 1, padding='same', name='Conv3d_3c_0a_1x1')\n",
        "\n",
        "    branch_1 = conv3d_bn(x, 128, 1, 1, 1, padding='same', name='Conv3d_3c_1a_1x1')\n",
        "    branch_1 = conv3d_bn(branch_1, 192, 3, 3, 3, padding='same', name='Conv3d_3c_1b_3x3')\n",
        "\n",
        "    branch_2 = conv3d_bn(x, 32, 1, 1, 1, padding='same', name='Conv3d_3c_2a_1x1')\n",
        "    branch_2 = conv3d_bn(branch_2, 96, 3, 3, 3, padding='same', name='Conv3d_3c_2b_3x3')\n",
        "\n",
        "    branch_3 = MaxPooling3D((3, 3, 3), strides=(1, 1, 1), padding='same', name='MaxPool2d_3c_3a_3x3')(x)\n",
        "    branch_3 = conv3d_bn(branch_3, 64, 1, 1, 1, padding='same', name='Conv3d_3c_3b_1x1')\n",
        "\n",
        "    x = layers.concatenate(\n",
        "        [branch_0, branch_1, branch_2, branch_3],\n",
        "        axis=channel_axis,\n",
        "        name='Mixed_3c')\n",
        "\n",
        "\n",
        "    # Downsampling (spatial and temporal)\n",
        "    x = MaxPooling3D((3, 3, 3), strides=(2, 2, 2), padding='same', name='MaxPool2d_4a_3x3')(x)\n",
        "\n",
        "    # Mixed 4b\n",
        "    branch_0 = conv3d_bn(x, 192, 1, 1, 1, padding='same', name='Conv3d_4b_0a_1x1')\n",
        "\n",
        "    branch_1 = conv3d_bn(x, 96, 1, 1, 1, padding='same', name='Conv3d_4b_1a_1x1')\n",
        "    branch_1 = conv3d_bn(branch_1, 208, 3, 3, 3, padding='same', name='Conv3d_4b_1b_3x3')\n",
        "\n",
        "    branch_2 = conv3d_bn(x, 16, 1, 1, 1, padding='same', name='Conv3d_4b_2a_1x1')\n",
        "    branch_2 = conv3d_bn(branch_2, 48, 3, 3, 3, padding='same', name='Conv3d_4b_2b_3x3')\n",
        "\n",
        "    branch_3 = MaxPooling3D((3, 3, 3), strides=(1, 1, 1), padding='same', name='MaxPool2d_4b_3a_3x3')(x)\n",
        "    branch_3 = conv3d_bn(branch_3, 64, 1, 1, 1, padding='same', name='Conv3d_4b_3b_1x1')\n",
        "\n",
        "    x = layers.concatenate(\n",
        "        [branch_0, branch_1, branch_2, branch_3],\n",
        "        axis=channel_axis,\n",
        "        name='Mixed_4b')\n",
        "\n",
        "    # Mixed 4c\n",
        "    branch_0 = conv3d_bn(x, 160, 1, 1, 1, padding='same', name='Conv3d_4c_0a_1x1')\n",
        "\n",
        "    branch_1 = conv3d_bn(x, 112, 1, 1, 1, padding='same', name='Conv3d_4c_1a_1x1')\n",
        "    branch_1 = conv3d_bn(branch_1, 224, 3, 3, 3, padding='same', name='Conv3d_4c_1b_3x3')\n",
        "\n",
        "    branch_2 = conv3d_bn(x, 24, 1, 1, 1, padding='same', name='Conv3d_4c_2a_1x1')\n",
        "    branch_2 = conv3d_bn(branch_2, 64, 3, 3, 3, padding='same', name='Conv3d_4c_2b_3x3')\n",
        "\n",
        "    branch_3 = MaxPooling3D((3, 3, 3), strides=(1, 1, 1), padding='same', name='MaxPool2d_4c_3a_3x3')(x)\n",
        "    branch_3 = conv3d_bn(branch_3, 64, 1, 1, 1, padding='same', name='Conv3d_4c_3b_1x1')\n",
        "\n",
        "    x = layers.concatenate(\n",
        "        [branch_0, branch_1, branch_2, branch_3],\n",
        "        axis=channel_axis,\n",
        "        name='Mixed_4c')\n",
        "\n",
        "    # Mixed 4d\n",
        "    branch_0 = conv3d_bn(x, 128, 1, 1, 1, padding='same', name='Conv3d_4d_0a_1x1')\n",
        "\n",
        "    branch_1 = conv3d_bn(x, 128, 1, 1, 1, padding='same', name='Conv3d_4d_1a_1x1')\n",
        "    branch_1 = conv3d_bn(branch_1, 256, 3, 3, 3, padding='same', name='Conv3d_4d_1b_3x3')\n",
        "\n",
        "    branch_2 = conv3d_bn(x, 24, 1, 1, 1, padding='same', name='Conv3d_4d_2a_1x1')\n",
        "    branch_2 = conv3d_bn(branch_2, 64, 3, 3, 3, padding='same', name='Conv3d_4d_2b_3x3')\n",
        "\n",
        "    branch_3 = MaxPooling3D((3, 3, 3), strides=(1, 1, 1), padding='same', name='MaxPool2d_4d_3a_3x3')(x)\n",
        "    branch_3 = conv3d_bn(branch_3, 64, 1, 1, 1, padding='same', name='Conv3d_4d_3b_1x1')\n",
        "\n",
        "    x = layers.concatenate(\n",
        "        [branch_0, branch_1, branch_2, branch_3],\n",
        "        axis=channel_axis,\n",
        "        name='Mixed_4d')\n",
        "\n",
        "    # Mixed 4e\n",
        "    branch_0 = conv3d_bn(x, 112, 1, 1, 1, padding='same', name='Conv3d_4e_0a_1x1')\n",
        "\n",
        "    branch_1 = conv3d_bn(x, 144, 1, 1, 1, padding='same', name='Conv3d_4e_1a_1x1')\n",
        "    branch_1 = conv3d_bn(branch_1, 288, 3, 3, 3, padding='same', name='Conv3d_4e_1b_3x3')\n",
        "\n",
        "    branch_2 = conv3d_bn(x, 32, 1, 1, 1, padding='same', name='Conv3d_4e_2a_1x1')\n",
        "    branch_2 = conv3d_bn(branch_2, 64, 3, 3, 3, padding='same', name='Conv3d_4e_2b_3x3')\n",
        "\n",
        "    branch_3 = MaxPooling3D((3, 3, 3), strides=(1, 1, 1), padding='same', name='MaxPool2d_4e_3a_3x3')(x)\n",
        "    branch_3 = conv3d_bn(branch_3, 64, 1, 1, 1, padding='same', name='Conv3d_4e_3b_1x1')\n",
        "\n",
        "    x = layers.concatenate(\n",
        "        [branch_0, branch_1, branch_2, branch_3],\n",
        "        axis=channel_axis,\n",
        "        name='Mixed_4e')\n",
        "\n",
        "    # Mixed 4f\n",
        "    branch_0 = conv3d_bn(x, 256, 1, 1, 1, padding='same', name='Conv3d_4f_0a_1x1')\n",
        "\n",
        "    branch_1 = conv3d_bn(x, 160, 1, 1, 1, padding='same', name='Conv3d_4f_1a_1x1')\n",
        "    branch_1 = conv3d_bn(branch_1, 320, 3, 3, 3, padding='same', name='Conv3d_4f_1b_3x3')\n",
        "\n",
        "    branch_2 = conv3d_bn(x, 32, 1, 1, 1, padding='same', name='Conv3d_4f_2a_1x1')\n",
        "    branch_2 = conv3d_bn(branch_2, 128, 3, 3, 3, padding='same', name='Conv3d_4f_2b_3x3')\n",
        "\n",
        "    branch_3 = MaxPooling3D((3, 3, 3), strides=(1, 1, 1), padding='same', name='MaxPool2d_4f_3a_3x3')(x)\n",
        "    branch_3 = conv3d_bn(branch_3, 128, 1, 1, 1, padding='same', name='Conv3d_4f_3b_1x1')\n",
        "\n",
        "    x = layers.concatenate(\n",
        "        [branch_0, branch_1, branch_2, branch_3],\n",
        "        axis=channel_axis,\n",
        "        name='Mixed_4f')\n",
        "\n",
        "\n",
        "    # Downsampling (spatial and temporal)\n",
        "    x = MaxPooling3D((2, 2, 2), strides=(2, 2, 2), padding='same', name='MaxPool2d_5a_2x2')(x)\n",
        "\n",
        "    # Mixed 5b\n",
        "    branch_0 = conv3d_bn(x, 256, 1, 1, 1, padding='same', name='Conv3d_5b_0a_1x1')\n",
        "\n",
        "    branch_1 = conv3d_bn(x, 160, 1, 1, 1, padding='same', name='Conv3d_5b_1a_1x1')\n",
        "    branch_1 = conv3d_bn(branch_1, 320, 3, 3, 3, padding='same', name='Conv3d_5b_1b_3x3')\n",
        "\n",
        "    branch_2 = conv3d_bn(x, 32, 1, 1, 1, padding='same', name='Conv3d_5b_2a_1x1')\n",
        "    branch_2 = conv3d_bn(branch_2, 128, 3, 3, 3, padding='same', name='Conv3d_5b_2b_3x3')\n",
        "\n",
        "    branch_3 = MaxPooling3D((3, 3, 3), strides=(1, 1, 1), padding='same', name='MaxPool2d_5b_3a_3x3')(x)\n",
        "    branch_3 = conv3d_bn(branch_3, 128, 1, 1, 1, padding='same', name='Conv3d_5b_3b_1x1')\n",
        "\n",
        "    x = layers.concatenate(\n",
        "        [branch_0, branch_1, branch_2, branch_3],\n",
        "        axis=channel_axis,\n",
        "        name='Mixed_5b')\n",
        "\n",
        "    # Mixed 5c\n",
        "    branch_0 = conv3d_bn(x, 384, 1, 1, 1, padding='same', name='Conv3d_5c_0a_1x1')\n",
        "\n",
        "    branch_1 = conv3d_bn(x, 192, 1, 1, 1, padding='same', name='Conv3d_5c_1a_1x1')\n",
        "    branch_1 = conv3d_bn(branch_1, 384, 3, 3, 3, padding='same', name='Conv3d_5c_1b_3x3')\n",
        "\n",
        "    branch_2 = conv3d_bn(x, 48, 1, 1, 1, padding='same', name='Conv3d_5c_2a_1x1')\n",
        "    branch_2 = conv3d_bn(branch_2, 128, 3, 3, 3, padding='same', name='Conv3d_5c_2b_3x3')\n",
        "\n",
        "    branch_3 = MaxPooling3D((3, 3, 3), strides=(1, 1, 1), padding='same', name='MaxPool2d_5c_3a_3x3')(x)\n",
        "    branch_3 = conv3d_bn(branch_3, 128, 1, 1, 1, padding='same', name='Conv3d_5c_3b_1x1')\n",
        "\n",
        "    x = layers.concatenate(\n",
        "        [branch_0, branch_1, branch_2, branch_3],\n",
        "        axis=channel_axis,\n",
        "        name='Mixed_5c')\n",
        "\n",
        "    if include_top:\n",
        "        # Classification block\n",
        "        x = AveragePooling3D((2, 7, 7), strides=(1, 1, 1), padding='valid', name='global_avg_pool')(x)\n",
        "        x = Dropout(dropout_prob)(x)\n",
        "\n",
        "        x = conv3d_bn(x, classes, 1, 1, 1, padding='same', \n",
        "                use_bias=True, use_activation_fn=False, use_bn=False, name='Conv3d_6a_1x1')\n",
        " \n",
        "        num_frames_remaining = int(x.shape[1])\n",
        "        x = Reshape((num_frames_remaining, classes))(x)\n",
        "\n",
        "        # logits (raw scores for each class)\n",
        "        x = Lambda(lambda x: K.mean(x, axis=1, keepdims=False),\n",
        "                   output_shape=lambda s: (s[0], s[2]))(x)\n",
        "\n",
        "        if not endpoint_logit:\n",
        "            x = Activation('softmax', name='prediction')(x)\n",
        "    else:\n",
        "        h = int(x.shape[2])\n",
        "        w = int(x.shape[3])\n",
        "        x = AveragePooling3D((2, h, w), strides=(1, 1, 1), padding='valid', name='global_avg_pool')(x)\n",
        "\n",
        "\n",
        "\n",
        "    inputs = img_input\n",
        "    # create model\n",
        "    model = Model(inputs, x, name='i3d_inception')\n",
        "\n",
        "    # load weights\n",
        "    if weights in WEIGHTS_NAME:\n",
        "        if weights == WEIGHTS_NAME[0]:   # rgb_kinetics_only\n",
        "            if include_top:\n",
        "                weights_url = WEIGHTS_PATH['rgb_kinetics_only']\n",
        "                model_name = 'i3d_inception_rgb_kinetics_only.h5'\n",
        "            else:\n",
        "                weights_url = WEIGHTS_PATH_NO_TOP['rgb_kinetics_only']\n",
        "                model_name = 'i3d_inception_rgb_kinetics_only_no_top.h5'\n",
        "\n",
        "        elif weights == WEIGHTS_NAME[1]: # flow_kinetics_only\n",
        "            if include_top:\n",
        "                weights_url = WEIGHTS_PATH['flow_kinetics_only']\n",
        "                model_name = 'i3d_inception_flow_kinetics_only.h5'\n",
        "            else:\n",
        "                weights_url = WEIGHTS_PATH_NO_TOP['flow_kinetics_only']\n",
        "                model_name = 'i3d_inception_flow_kinetics_only_no_top.h5'\n",
        "\n",
        "        elif weights == WEIGHTS_NAME[2]: # rgb_imagenet_and_kinetics\n",
        "            if include_top:\n",
        "                weights_url = WEIGHTS_PATH['rgb_imagenet_and_kinetics']\n",
        "                model_name = 'i3d_inception_rgb_imagenet_and_kinetics.h5'\n",
        "            else:\n",
        "                weights_url = WEIGHTS_PATH_NO_TOP['rgb_imagenet_and_kinetics']\n",
        "                model_name = 'i3d_inception_rgb_imagenet_and_kinetics_no_top.h5'\n",
        "\n",
        "        elif weights == WEIGHTS_NAME[3]: # flow_imagenet_and_kinetics\n",
        "            if include_top:\n",
        "                weights_url = WEIGHTS_PATH['flow_imagenet_and_kinetics']\n",
        "                model_name = 'i3d_inception_flow_imagenet_and_kinetics.h5'\n",
        "            else:\n",
        "                weights_url = WEIGHTS_PATH_NO_TOP['flow_imagenet_and_kinetics']\n",
        "                model_name = 'i3d_inception_flow_imagenet_and_kinetics_no_top.h5'\n",
        "\n",
        "        downloaded_weights_path = get_file(model_name, weights_url, cache_subdir='models')\n",
        "        model.load_weights(downloaded_weights_path)\n",
        "\n",
        "        if K.backend() == 'theano':\n",
        "            layer_utils.convert_all_kernels_in_model(model)\n",
        "\n",
        "        if K.image_data_format() == 'channels_first' and K.backend() == 'tensorflow':\n",
        "            warnings.warn('You are using the TensorFlow backend, yet you '\n",
        "                          'are using the Theano '\n",
        "                          'image data format convention '\n",
        "                          '(`image_data_format=\"channels_first\"`). '\n",
        "                          'For best performance, set '\n",
        "                          '`image_data_format=\"channels_last\"` in '\n",
        "                          'your keras config '\n",
        "                          'at ~/.keras/keras.json.')\n",
        "\n",
        "    elif weights is not None:\n",
        "        model.load_weights(weights)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "uSq1jrg8xBDD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_i3d_feature_extract = Inception_Inflated3d(include_top=False,\n",
        "                weights='rgb_imagenet_and_kinetics',\n",
        "                input_shape=(16,224,224,3),\n",
        "                dropout_prob=0.0,\n",
        "                endpoint_logit=True,\n",
        "                classes=400)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBCsNIs7_mWQ",
        "outputId": "7e39d496-1123-4ea4-837f-60005f354bd3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/dlpbc/keras-kinetics-i3d/releases/download/v0.2/rgb_inception_i3d_imagenet_and_kinetics_tf_dim_ordering_tf_kernels_no_top.h5\n",
            "49602560/49595336 [==============================] - 6s 0us/step\n",
            "49610752/49595336 [==============================] - 6s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_i3d_feature_extract.summary()"
      ],
      "metadata": {
        "id": "U1ylImNbB-XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBYYYvCbcJTm"
      },
      "source": [
        "# Extract Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNtaMomVcbP7"
      },
      "source": [
        "## Normal Videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "GJs9rMnoQTm5"
      },
      "outputs": [],
      "source": [
        "normal_videos = os.listdir(normal_videos_path)\n",
        "normal_videos.sort()\n",
        "chunk = list(divide_chunks(normal_videos, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhPxsMW0yQUu"
      },
      "source": [
        "### I3d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcInkqrOcegu"
      },
      "outputs": [],
      "source": [
        "print(\"Processing normal videos...\")\n",
        "for vid_name in chunk[7]:\n",
        "    print(\"Processing {}\".format(vid_name))\n",
        "    vid_path = os.path.join(normal_videos_path, vid_name)\n",
        "    feats_path = os.path.join(\n",
        "        raw_normal_train_features_i3d, vid_name[:-9] + \".npy\"\n",
        "    )\n",
        "\n",
        "    clips, frames = get_video_clips(vid_path, i3d_frame_count)\n",
        "\n",
        "    # Remove last clip if number of frames is not equal to 16\n",
        "    if frames % 16 != 0:\n",
        "        clips = clips[:-1]\n",
        "    \n",
        "    print(frames)\n",
        "    print(len(clips))\n",
        "    if frames > 1700 or frames < 1500:\n",
        "      continue\n",
        "\n",
        "    prep_clips = [preprocess_input_i3d(np.array(clip)) for clip in clips]\n",
        "    prep_clips = np.vstack(prep_clips)\n",
        "\n",
        "    features = model_i3d_feature_extract(prep_clips)\n",
        "\n",
        "    features = np.reshape(features, (features.shape[0], features.shape[4]))\n",
        "    features = sklearn.preprocessing.normalize(features, axis=1)\n",
        "    del prep_clips\n",
        "    \n",
        "    with open(feats_path,\"wb\") as f: \n",
        "        np.save(f, features)\n",
        "      \n",
        "    del features\n",
        "    gc.collect(generation=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_abnormal_train_features_i3d_list=os.listdir(raw_normal_train_features_i3d)\n",
        "for filename in os.listdir(raw_normal_train_features_i3d):\n",
        "    print(\"Processing {}\".format(filename))\n",
        "    raw_file_path = os.path.join(\n",
        "        raw_normal_train_features_i3d, filename\n",
        "    )\n",
        "    processed_file_path = os.path.join(\n",
        "        processed_normal_train_features_i3d, filename\n",
        "    )\n",
        "    with open(raw_file_path, \"rb\") as f:\n",
        "        feats = np.load(f, allow_pickle=True)\n",
        "\n",
        "    try:\n",
        "        new_feats = transform_into_segments(feats)\n",
        "        with open(processed_file_path, \"wb\") as f:\n",
        "            np.save(f, new_feats, allow_pickle=True)\n",
        "    except RuntimeError:\n",
        "        print(\"Video {} too short\".format(filename))"
      ],
      "metadata": {
        "id": "NMUbN_dqNzZo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "745d5126-23c7-4182-92f1-6656ed63e938"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Normal_Videos154.npy\n",
            "Processing Normal_Videos156.npy\n",
            "Processing Normal_Videos159.npy\n",
            "Processing Normal_Videos162.npy\n",
            "Processing Normal_Videos170.npy\n",
            "Processing Normal_Videos173.npy\n",
            "Processing Normal_Videos178.npy\n",
            "Processing Normal_Videos204.npy\n",
            "Processing Normal_Videos209.npy\n",
            "Processing Normal_Videos215.npy\n",
            "Processing Normal_Videos218.npy\n",
            "Processing Normal_Videos_717.npy\n",
            "Processing Normal_Videos_725.npy\n",
            "Processing Normal_Videos_745.npy\n",
            "Video Normal_Videos_745.npy too short\n",
            "Processing Normal_Videos_778.npy\n",
            "Processing Normal_Videos_828.npy\n",
            "Processing Normal_Videos_831.npy\n",
            "Video Normal_Videos_831.npy too short\n",
            "Processing Normal_Videos_866.npy\n",
            "Processing Normal_Videos_867.npy\n",
            "Processing Normal_Videos_870.npy\n",
            "Processing Normal_Videos_872.npy\n",
            "Processing Normal_Videos_876.npy\n",
            "Video Normal_Videos_876.npy too short\n",
            "Processing Normal_Videos_878.npy\n",
            "Video Normal_Videos_878.npy too short\n",
            "Processing Normal_Videos_879.npy\n",
            "Processing Normal_Videos_881.npy\n",
            "Video Normal_Videos_881.npy too short\n",
            "Processing Normal_Videos_883.npy\n",
            "Video Normal_Videos_883.npy too short\n",
            "Processing Normal_Videos_885.npy\n",
            "Video Normal_Videos_885.npy too short\n",
            "Processing Normal_Videos_888.npy\n",
            "Processing Normal_Videos_897.npy\n",
            "Processing Normal_Videos_898.npy\n",
            "Processing Normal_Videos_899.npy\n",
            "Processing Normal_Videos_900.npy\n",
            "Processing Normal_Videos_901.npy\n",
            "Processing Normal_Videos_902.npy\n",
            "Processing Normal_Videos_903.npy\n",
            "Processing Normal_Videos_904.npy\n",
            "Processing Normal_Videos_905.npy\n",
            "Processing Normal_Videos_906.npy\n",
            "Processing Normal_Videos_907.npy\n",
            "Processing Normal_Videos_908.npy\n",
            "Processing Normal_Videos_909.npy\n",
            "Processing Normal_Videos_910.npy\n",
            "Processing Normal_Videos_911.npy\n",
            "Processing Normal_Videos_912.npy\n",
            "Processing Normal_Videos_913.npy\n",
            "Processing Normal_Videos_914.npy\n",
            "Processing Normal_Videos_915.npy\n",
            "Processing Normal_Videos_928.npy\n",
            "Processing Normal_Videos_929.npy\n",
            "Processing Normal_Videos_936.npy\n",
            "Processing Normal_Videos_937.npy\n",
            "Processing Normal_Videos_939.npy\n",
            "Processing Normal_Videos_943.npy\n",
            "Processing Normal_Videos104.npy\n",
            "Processing Normal_Videos105.npy\n",
            "Processing Normal_Videos112.npy\n",
            "Processing Normal_Videos117.npy\n",
            "Processing Normal_Videos118.npy\n",
            "Processing Normal_Videos124.npy\n",
            "Processing Normal_Videos120.npy\n",
            "Processing Normal_Videos146.npy\n",
            "Processing Normal_Videos148.npy\n",
            "Processing Normal_Videos149.npy\n",
            "Processing Normal_Videos166.npy\n",
            "Processing Normal_Videos167.npy\n",
            "Processing Normal_Videos169.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(os.listdir('/content/drive/MyDrive/Anomaly Recognition/Pre-processed Normal Video features i3d'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpAHkrYcTAFQ",
        "outputId": "0e64badf-b5cf-4c5f-99eb-c22dc9c53d8b"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3jDISdJg5Ln"
      },
      "source": [
        "## Anomaly Videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-4sXcsScwyL0"
      },
      "outputs": [],
      "source": [
        "abnormal_videos = os.listdir(abnormal_videos_path)\n",
        "abnormal_videos.sort()\n",
        "chunk = list(divide_chunks(abnormal_videos, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C3d"
      ],
      "metadata": {
        "id": "lKzL2lYHioRU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7Y5IN98cjqd"
      },
      "outputs": [],
      "source": [
        "print(\"Processing abnormal videos in chunks...\")\n",
        "for vid_name in chunk:\n",
        "    print(\"Processing {}\".format(vid_name))\n",
        "    vid_path = os.path.join(abnormal_videos_path, vid_name)\n",
        "    feats_path = os.path.join(\n",
        "        raw_abnormal_train_features_c3d, vid_name[:-9] + \".npy\"\n",
        "    )\n",
        "\n",
        "    clips, frames = get_video_clips(vid_path, c3d_frame_count)\n",
        "    print(frames)\n",
        "    if frames > 8000:\n",
        "      continue\n",
        "    # Remove last clip if number of frames is not equal to 16\n",
        "    if frames % 16 != 0:\n",
        "        clips = clips[:-1]\n",
        "\n",
        "    prep_clips = [preprocess_input_c3d(np.array(clip)) for clip in clips]\n",
        "    prep_clips = np.vstack(prep_clips)\n",
        "\n",
        "    features = feature_extractor.predict(prep_clips)\n",
        "    del prep_clips\n",
        "    features = sklearn.preprocessing.normalize(features, axis=1)   \n",
        "    with open(feats_path, \"wb\") as f:\n",
        "        np.save(f, features)\n",
        "    del features\n",
        "    gc.collect(generation=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERjzJ91fhwcD"
      },
      "outputs": [],
      "source": [
        "raw_abnormal_train_features_c3d_list=os.listdir(raw_abnormal_train_features_c3d)\n",
        "for filename in os.listdir(raw_abnormal_train_features_c3d):\n",
        "    print(\"Processing {}\".format(filename))\n",
        "    raw_file_path = os.path.join(\n",
        "        raw_abnormal_train_features_c3d, filename\n",
        "    )\n",
        "    processed_file_path = os.path.join(\n",
        "        processed_abnormal_train_features_c3d, filename\n",
        "    )\n",
        "    with open(raw_file_path, \"rb\") as f:\n",
        "        feats = np.load(f, allow_pickle=True)\n",
        "\n",
        "    try:\n",
        "        new_feats = transform_into_segments(feats)\n",
        "        with open(processed_file_path, \"wb\") as f:\n",
        "            np.save(f, new_feats, allow_pickle=True)\n",
        "    except RuntimeError:\n",
        "        print(\"Video {} too short\".format(filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### i3d"
      ],
      "metadata": {
        "id": "dvv1W7mPisHb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyA980pWisHc"
      },
      "outputs": [],
      "source": [
        "print(\"Processing abnormal videos in chunks...\")\n",
        "for vid_name in chunk[12]:\n",
        "    print(\"Processing {}\".format(vid_name))\n",
        "    vid_path = os.path.join(abnormal_videos_path, vid_name)\n",
        "    feats_path = os.path.join(\n",
        "        raw_abnormal_train_features_i3d, vid_name[:-9] + \".npy\"\n",
        "    )\n",
        "\n",
        "    clips, frames = get_video_clips(vid_path, i3d_frame_count)\n",
        "    print(frames)\n",
        "    if frames > 4000 or frames < 2000:\n",
        "      continue\n",
        "    # Remove last clip if number of frames is not equal to 16\n",
        "    if frames % 16 != 0:\n",
        "        clips = clips[:-1]\n",
        "\n",
        "    prep_clips = [preprocess_input_i3d(np.array(clip)) for clip in clips]\n",
        "    prep_clips = np.vstack(prep_clips)\n",
        "\n",
        "    features = model_i3d_feature_extract.predict(prep_clips)\n",
        "    features = np.reshape(features, (features.shape[0], features.shape[4]))\n",
        "    features = sklearn.preprocessing.normalize(features, axis=1)\n",
        "    del prep_clips \n",
        "    with open(feats_path, \"wb\") as f:\n",
        "        np.save(f, features)\n",
        "    del features\n",
        "    gc.collect(generation=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_abnormal_train_features_c3d_list=os.listdir(raw_abnormal_train_features_i3d)\n",
        "for filename in os.listdir(raw_abnormal_train_features_i3d):\n",
        "    print(\"Processing {}\".format(filename))\n",
        "    raw_file_path = os.path.join(\n",
        "        raw_abnormal_train_features_i3d, filename\n",
        "    )\n",
        "    processed_file_path = os.path.join(\n",
        "        processed_abnormal_train_features_i3d, filename\n",
        "    )\n",
        "    with open(raw_file_path, \"rb\") as f:\n",
        "        feats = np.load(f, allow_pickle=True)\n",
        "\n",
        "    try:\n",
        "        new_feats = transform_into_segments(feats)\n",
        "        with open(processed_file_path, \"wb\") as f:\n",
        "            np.save(f, new_feats, allow_pickle=True)\n",
        "    except RuntimeError:\n",
        "        print(\"Video {} too short\".format(filename))"
      ],
      "metadata": {
        "id": "J-Cdsc4-SxMq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c44bd7b-4b1f-49b2-b71b-1bcbf5ddc079"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Abuse041.npy\n",
            "Processing Abuse046.npy\n",
            "Processing Arson044.npy\n",
            "Processing Arson047.npy\n",
            "Processing Arson051.npy\n",
            "Processing Assault044.npy\n",
            "Processing Assault045.npy\n",
            "Processing Assault049.npy\n",
            "Processing Assault050.npy\n",
            "Processing Assault051.npy\n",
            "Processing Burglary050.npy\n",
            "Processing Burglary092.npy\n",
            "Processing Burglary093.npy\n",
            "Processing Burglary094.npy\n",
            "Processing Burglary098.npy\n",
            "Processing Explosion047.npy\n",
            "Processing Explosion048.npy\n",
            "Video Explosion048.npy too short\n",
            "Processing RoadAccidents143.npy\n",
            "Processing RoadAccidents145.npy\n",
            "Processing RoadAccidents146.npy\n",
            "Processing RoadAccidents150.npy\n",
            "Processing RoadAccidents151.npy\n",
            "Processing Robbery143.npy\n",
            "Processing Robbery144.npy\n",
            "Processing Robbery145.npy\n",
            "Processing Robbery147.npy\n",
            "Processing Robbery149.npy\n",
            "Processing Shooting027.npy\n",
            "Processing Shooting040.npy\n",
            "Processing Shooting041.npy\n",
            "Processing Shooting051.npy\n",
            "Processing Shooting053.npy\n",
            "Processing Shoplifting028.npy\n",
            "Processing Stealing105.npy\n",
            "Processing Stealing106.npy\n",
            "Processing Stealing114.npy\n",
            "Processing Vandalism040.npy\n",
            "Processing Vandalism050.npy\n",
            "Processing Arrest002.npy\n",
            "Processing Arrest032.npy\n",
            "Processing Arrest040.npy\n",
            "Processing Arson040.npy\n",
            "Processing Arrest038.npy\n",
            "Processing Abuse040.npy\n",
            "Processing Abuse043.npy\n",
            "Processing Abuse044.npy\n",
            "Processing Arrest041.npy\n",
            "Processing Arson041.npy\n",
            "Processing Explosion041.npy\n",
            "Processing Explosion042.npy\n",
            "Processing Explosion051.npy\n",
            "Processing Shoplifting047.npy\n",
            "Processing Shoplifting048.npy\n",
            "Processing Shoplifting049.npy\n",
            "Processing Shoplifting050.npy\n",
            "Processing Stealing104.npy\n",
            "Processing Stealing112.npy\n",
            "Processing Vandalism038.npy\n",
            "Processing Vandalism042.npy\n",
            "Processing Vandalism043.npy\n",
            "Processing Fighting042.npy\n",
            "Processing Fighting002.npy\n",
            "Processing Fighting003.npy\n",
            "Processing Fighting007.npy\n",
            "Processing Fighting009.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6Ij0gMT2V3B"
      },
      "source": [
        "# AI Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iQX04Gm5K4h"
      },
      "source": [
        "## Parameters for traning model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_yGf9V9h2-sV"
      },
      "outputs": [],
      "source": [
        "normal_list_c3d = os.listdir(processed_normal_train_features_c3d)\n",
        "normal_list_c3d.sort()\n",
        "abnormal_list_c3d = os.listdir(processed_abnormal_train_features_c3d)\n",
        "abnormal_list_c3d.sort()\n",
        "normal_list_i3d = os.listdir(processed_normal_train_features_i3d)\n",
        "normal_list_i3d.sort()\n",
        "abnormal_list_i3d = os.listdir(processed_abnormal_train_features_i3d)\n",
        "abnormal_list_i3d.sort()\n",
        "weights_path_c3d = output_dir + 'weights_c3d.mat'\n",
        "model_path_c3d = output_dir + 'model_c3d.json'\n",
        "weights_path_i3d = output_dir + 'weights_i3d.mat'\n",
        "model_path_i3d = output_dir + 'model_i3d.json'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbw458e65bDv"
      },
      "source": [
        "## DNN Model C3d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "zM_Pn3gH4Cu7"
      },
      "outputs": [],
      "source": [
        "def classifier_c3d_model():\n",
        "    \"\"\"Build the classifier\n",
        "    :returns: Classifier model\n",
        "    :rtype: keras.Model\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512, input_dim=4096, kernel_initializer='glorot_normal',\n",
        "                    kernel_regularizer=l2(0.001), activation='relu'))\n",
        "    model.add(Dropout(0.6))\n",
        "    model.add(Dense(32, kernel_initializer='glorot_normal',\n",
        "                    kernel_regularizer=l2(0.001)))\n",
        "    model.add(Dropout(0.6))\n",
        "    model.add(Dense(1, kernel_initializer='glorot_normal',\n",
        "                    kernel_regularizer=l2(0.001), activation='sigmoid'))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDCE66RXF3Sr"
      },
      "source": [
        "## DNN Model i3d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "KQaSN68xF3S2"
      },
      "outputs": [],
      "source": [
        "def classifier_i3d_model():\n",
        "    \"\"\"Build the classifier\n",
        "    :returns: Classifier model\n",
        "    :rtype: keras.Model\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512, input_dim=1024, kernel_initializer='glorot_normal',activation='relu'))\n",
        "    model.add(Dense(256, input_dim=512, kernel_initializer='glorot_normal', activation='relu'))\n",
        "    model.add(Dense(32, kernel_initializer='glorot_normal'))\n",
        "    model.add(Dense(1, kernel_initializer='glorot_normal', activation='sigmoid'))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDNVo47z5fMk"
      },
      "source": [
        "## Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "29XNys-822do"
      },
      "outputs": [],
      "source": [
        "def load_batch_train(normal_path, normal_list, abnormal_path, abnormal_list):\n",
        "\n",
        "    batchsize=60\n",
        "    n_exp = int(batchsize/2)\n",
        "\n",
        "    num_normal = len(normal_list)\n",
        "    num_abnormal = len(abnormal_list)\n",
        "\n",
        "    abnor_list_idx = np.random.permutation(num_abnormal)\n",
        "    abnor_list = abnor_list_idx[:n_exp]\n",
        "    norm_list_idx = np.random.permutation(num_normal)\n",
        "    norm_list = norm_list_idx[:n_exp]\n",
        "\n",
        "    abnormal_feats = []\n",
        "    for video_idx in abnor_list:\n",
        "        video_path = os.path.join(abnormal_path, abnormal_list[video_idx])\n",
        "        with open(video_path, \"rb\") as f:\n",
        "            feats = np.load(f)\n",
        "        abnormal_feats.append(feats)\n",
        "\n",
        "    normal_feats = []\n",
        "    for video_idx in norm_list:\n",
        "        video_path = os.path.join(normal_path, normal_list[video_idx])\n",
        "        with open(video_path, \"rb\") as f:\n",
        "            feats = np.load(f)\n",
        "        normal_feats.append(feats)\n",
        "\n",
        "\n",
        "    all_feats = np.vstack((*abnormal_feats, *normal_feats))\n",
        "    all_labels = np.zeros(32*batchsize, dtype='uint8')\n",
        "\n",
        "    all_labels[:32*n_exp] = 1\n",
        "\n",
        "    return  all_feats, all_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfNhzIxo5hXA"
      },
      "source": [
        "## Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "KIHGFHum26-U"
      },
      "outputs": [],
      "source": [
        "def custom_objective(y_true, y_pred):\n",
        "\n",
        "    y_true = K.reshape(y_true, [-1])\n",
        "    y_pred = K.reshape(y_pred, [-1])\n",
        "    n_seg = 32\n",
        "    nvid = 60\n",
        "    n_exp = int(nvid / 2)\n",
        "\n",
        "    max_scores_list = []\n",
        "    z_scores_list = []\n",
        "    temporal_constrains_list = []\n",
        "    sparsity_constrains_list = []\n",
        "\n",
        "    for i in range(0, n_exp, 1):\n",
        "\n",
        "        video_predictions = y_pred[i*n_seg:(i+1)*n_seg]\n",
        "\n",
        "        max_scores_list.append(K.max(video_predictions))\n",
        "        temporal_constrains_list.append(\n",
        "            K.sum(K.pow(video_predictions[1:] - video_predictions[:-1], 2))\n",
        "        )\n",
        "        sparsity_constrains_list.append(K.sum(video_predictions))\n",
        "\n",
        "    for j in range(n_exp, 2*n_exp, 1):\n",
        "\n",
        "        video_predictions = y_pred[j*n_seg:(j+1)*n_seg]\n",
        "        max_scores_list.append(K.max(video_predictions))\n",
        "\n",
        "    max_scores = K.stack(max_scores_list)\n",
        "    temporal_constrains = K.stack(temporal_constrains_list)\n",
        "    sparsity_constrains = K.stack(sparsity_constrains_list)\n",
        "\n",
        "    for ii in range(0, n_exp, 1):\n",
        "        max_z = K.maximum(1 - max_scores[:n_exp] + max_scores[n_exp+ii], 0)\n",
        "        z_scores_list.append(K.sum(max_z))\n",
        "\n",
        "    z_scores = K.stack(z_scores_list)\n",
        "    z = K.mean(z_scores)\n",
        "\n",
        "    return z + \\\n",
        "        0.00008*K.sum(temporal_constrains) + \\\n",
        "        0.00008*K.sum(sparsity_constrains)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hv0MOr8F5jPo"
      },
      "source": [
        "## Model Initialization "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "nDCVxZmc5D7m"
      },
      "outputs": [],
      "source": [
        "#Create Full connected Model\n",
        "model_c3d = classifier_c3d_model()\n",
        "model_i3d = classifier_i3d_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2s89ou05m3o"
      },
      "source": [
        "## Compile model and define some parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZQ5hxFy5DzC"
      },
      "outputs": [],
      "source": [
        "adagrad = tf.keras.optimizers.Adagrad(lr=0.001, epsilon=1e-08)\n",
        "model_c3d.compile(loss=custom_objective, optimizer=adagrad)\n",
        "model_i3d.compile(loss=custom_objective, optimizer=adagrad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "JPGLFVFq5IsV"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(output_dir):\n",
        "       os.makedirs(output_dir)\n",
        "\n",
        "loss_graph =[]\n",
        "num_iters = 4000\n",
        "total_iterations = 0\n",
        "batchsize=32\n",
        "time_before = datetime.now()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjsZmv-15rS9"
      },
      "source": [
        "## Train Model C3d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "-Cl9ldI3Pd6Z"
      },
      "outputs": [],
      "source": [
        "Loss, epochs = [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8fLIrPx4rTW"
      },
      "outputs": [],
      "source": [
        "for it_num in range(num_iters):\n",
        "  inputs, targets = load_batch_train(\n",
        "        processed_normal_train_features_c3d, normal_list_c3d, processed_abnormal_train_features_c3d, abnormal_list_c3d\n",
        "  )\n",
        "  batch_loss = model_c3d.train_on_batch(inputs, targets)\n",
        "  loss_graph = np.hstack((loss_graph, batch_loss))\n",
        "  total_iterations += 1\n",
        "  if total_iterations % 20 == 0:\n",
        "    epochs.append(total_iterations)\n",
        "    Loss.append(batch_loss)\n",
        "    print (\"Iteration={} took: {}, loss: {}\".format(\n",
        "            total_iterations, datetime.now() - time_before, batch_loss)\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CK3-dNp9RyWo"
      },
      "outputs": [],
      "source": [
        "plt.plot(epochs, Loss)\n",
        "plt.xlabel('epochs') \n",
        "plt.ylabel('loss') \n",
        "  \n",
        "# displaying the title\n",
        "plt.title(\"Loss vs epochs graph\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA8RKCDz9lC8"
      },
      "source": [
        "## Train Model I3d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga_DVp_59lC9"
      },
      "outputs": [],
      "source": [
        "Loss, epochs = [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0a981fd-83b6-4eeb-a5ca-501ac2dc054c",
        "id": "dnBilzmA9lC9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration=240 took: 0:01:31.529735, loss: 2.269014596939087\n"
          ]
        }
      ],
      "source": [
        "for it_num in range(num_iters):\n",
        "  inputs, targets = load_batch_train(\n",
        "        processed_normal_train_features_i3d, normal_list_i3d, processed_abnormal_train_features_i3d, abnormal_list_i3d\n",
        "  )\n",
        "  batch_loss = model_i3d.train_on_batch(inputs, targets)\n",
        "  loss_graph = np.hstack((loss_graph, batch_loss))\n",
        "  total_iterations += 1\n",
        "  if total_iterations % 20 == 0:\n",
        "    epochs.append(total_iterations)\n",
        "    Loss.append(batch_loss)\n",
        "    print (\"Iteration={} took: {}, loss: {}\".format(\n",
        "            total_iterations, datetime.now() - time_before, batch_loss)\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "52452cc0-ecf8-412a-b941-12432871c73c",
        "id": "0owx7Pty9lC-"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(epochs, Loss)\n",
        "plt.xlabel('epochs') \n",
        "plt.ylabel('loss') \n",
        "  \n",
        "# displaying the title\n",
        "plt.title(\"Loss vs epochs graph\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKJT0Uok5uaO"
      },
      "source": [
        "## Saving model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 325,
      "metadata": {
        "id": "X7qlEoPC21Ue"
      },
      "outputs": [],
      "source": [
        "def load_model(json_path):\n",
        "    model = model_from_json(open(json_path).read())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "metadata": {
        "id": "Y1Sh5ECH2XA7"
      },
      "outputs": [],
      "source": [
        "def save_model(model, json_path, weight_path):\n",
        "    json_string = model.to_json()\n",
        "    open(json_path, 'w').write(json_string)\n",
        "    dict = {}\n",
        "    i = 0\n",
        "    for layer in model.layers:\n",
        "        weights = layer.get_weights()\n",
        "        my_list = np.zeros(len(weights), dtype=np.object)\n",
        "        my_list[:] = weights\n",
        "        dict[str(i)] = my_list\n",
        "        i += 1\n",
        "    scipy.io.savemat(weight_path, dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I3d"
      ],
      "metadata": {
        "id": "vBuBadei94_w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uU5PJ9XL5CA_"
      },
      "outputs": [],
      "source": [
        "print(\"Train Successful - Model saved\")\n",
        "save_model(model_i3d, model_path_i3d, weights_path_i3d)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C3d"
      ],
      "metadata": {
        "id": "5eiM8xdi98z-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKQaf0uj98z_"
      },
      "outputs": [],
      "source": [
        "print(\"Train Successful - Model saved\")\n",
        "save_model(model_c3d, model_path_c3d, weights_path_c3d)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ],
      "metadata": {
        "id": "PFbc5TLGbV2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_dict(dict2):\n",
        "    \"\"\"Prepare the dictionary of weights to be loaded by the network\n",
        "    :param dict2: Dictionary to format\n",
        "    :returns: The dictionary properly formatted\n",
        "    :rtype: dict\n",
        "    \"\"\"\n",
        "    dict = {}\n",
        "    for i in range(len(dict2)):\n",
        "        if str(i) in dict2:\n",
        "            if dict2[str(i)].shape == (0, 0):\n",
        "                dict[str(i)] = dict2[str(i)]\n",
        "            else:\n",
        "                weights = dict2[str(i)][0]\n",
        "                weights2 = []\n",
        "                for weight in weights:\n",
        "                    if weight.shape in [(1, x) for x in range(0, 5000)]:\n",
        "                        weights2.append(weight[0])\n",
        "                    else:\n",
        "                        weights2.append(weight)\n",
        "                dict[str(i)] = weights2\n",
        "    return dict"
      ],
      "metadata": {
        "id": "4z050ds-aLeH"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_weights(model, weights_file):\n",
        "    \"\"\"Loads the pretrained weights into the network architecture\n",
        "    :param model: keras model of the network\n",
        "    :param weights_file: Path to the weights file\n",
        "    :returns: The input model with the weights properly loaded\n",
        "    :rtype: keras.model\n",
        "    \"\"\"\n",
        "    dict2 = sio.loadmat(weights_file)\n",
        "    dict = conv_dict(dict2)\n",
        "    i = 0\n",
        "    for layer in model.layers:\n",
        "        weights = dict[str(i)]\n",
        "        layer.set_weights(weights)\n",
        "        i += 1\n",
        "    return model"
      ],
      "metadata": {
        "id": "BUxaWT8gbd4G"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier_model(model, weights_path):\n",
        "    \"\"\"Build the classifier and load the pretrained weights\n",
        "    :returns:\n",
        "    :rtype:\n",
        "    \"\"\"\n",
        "    model = load_weights(model, weights_path)\n",
        "    return model"
      ],
      "metadata": {
        "id": "34AksG0nbmxr"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_c3d = build_classifier_model(model_c3d, weights_path_c3d)\n",
        "model_i3d = build_classifier_model(model_i3d, weights_path_i3d)"
      ],
      "metadata": {
        "id": "dOpyeeFCcbUC"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-UOUFkiVkBj"
      },
      "source": [
        "# Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "j0V4xqxLT5Zz"
      },
      "outputs": [],
      "source": [
        "def visualize_clip(clip, convert_bgr=False, save_gif=False, file_path=None):\n",
        "    num_frames = len(clip)\n",
        "    fig, ax = plt.subplots()\n",
        "    fig.set_tight_layout(True)\n",
        "\n",
        "    def update(i):\n",
        "        if convert_bgr:\n",
        "            frame = cv2.cvtColor(clip[i], cv2.COLOR_BGR2RGB)\n",
        "        else:\n",
        "            frame = clip[i]\n",
        "        plt.imshow(frame)\n",
        "        return plt\n",
        "\n",
        "    # FuncAnimation will call the 'update' function for each frame; here\n",
        "    # animating over 10 frames, with an interval of 20ms between frames.\n",
        "    anim = FuncAnimation(fig, update, frames=np.arange(0, num_frames), interval=1)\n",
        "    if save_gif:\n",
        "        anim.save(file_path, dpi=80, writer='pillow')\n",
        "    else:\n",
        "        # plt.show() will just loop the animation forever.\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "BBvgoDWWVloE"
      },
      "outputs": [],
      "source": [
        "def visualize_predictions(video_path, predictions, save_path):\n",
        "    frames = get_video_frames(video_path)\n",
        "    assert len(frames) == len(predictions)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    fig.set_tight_layout(True)\n",
        "\n",
        "    line = matplotlib.lines.Line2D([], [])\n",
        "\n",
        "    fig_frame = plt.subplot(2, 1, 1)\n",
        "    img = fig_frame.imshow(frames[0])\n",
        "    fig_prediction = plt.subplot(2, 1, 2)\n",
        "    fig_prediction.set_xlim(0, len(frames))\n",
        "    fig_prediction.set_ylim(0, 1.15)\n",
        "    fig_prediction.add_line(line)\n",
        "\n",
        "    def update(i):\n",
        "        frame = frames[i]\n",
        "        x = range(0, i)\n",
        "        y = predictions[0:i]\n",
        "        line.set_data(x, y)\n",
        "        img.set_data(frame)\n",
        "        return plt\n",
        "\n",
        "    # FuncAnimation will call the 'update' function for each frame; here\n",
        "    # animating over 10 frames, with an interval of 20ms between frames.\n",
        "\n",
        "    anim = FuncAnimation(fig, update, frames=np.arange(0, len(frames), 10), interval=1, repeat=False)\n",
        "\n",
        "    if save_path:\n",
        "        anim.save(save_path, dpi=200, writer=PillowWriter(fps=20))\n",
        "    else:\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I3d"
      ],
      "metadata": {
        "id": "klm0HkRW0LGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_demo_i3d(sample_video_path, video_name):\n",
        "    # read video\n",
        "    video_clips, num_frames = get_video_clips(sample_video_path, c3d_frame_count)\n",
        "\n",
        "    print(\"Number of clips in the video : \", len(video_clips))\n",
        "\n",
        "    # build models\n",
        "    feature_extractor = model_i3d_feature_extract\n",
        "    classifier_model = model_i3d\n",
        "\n",
        "    print(\"Models initialized\")\n",
        "\n",
        "    # extract features\n",
        "    rgb_features = []\n",
        "    for i, clip in enumerate(video_clips):\n",
        "        clip = np.array(clip)\n",
        "        if len(clip) < c3d_frame_count:\n",
        "            continue\n",
        "\n",
        "        clip = preprocess_input_i3d(clip)\n",
        "        rgb_feature = feature_extractor.predict(clip)[0]\n",
        "        rgb_features.append(rgb_feature)\n",
        "\n",
        "        print(\"Processed clip : \", i)\n",
        "\n",
        "    rgb_features = np.array(rgb_features)\n",
        "    rgb_features = np.reshape(rgb_features, (rgb_features.shape[0], rgb_features.shape[4]))\n",
        "    rgb_feature_bag = interpolate(rgb_features, features_per_bag)\n",
        "    \n",
        "    # classify using the trained classifier model\n",
        "    predictions = classifier_model.predict(rgb_feature_bag)\n",
        "\n",
        "    predictions = np.array(predictions).squeeze()\n",
        "\n",
        "    predictions = extrapolate(predictions, num_frames)\n",
        "    \n",
        "    save_path = os.path.join(output_folder_i3d, video_name + '.gif')\n",
        "    # visualize predictions\n",
        "    print('Executed Successfully - '+video_name + '.gif saved')\n",
        "    visualize_predictions(sample_video_path, predictions, save_path)"
      ],
      "metadata": {
        "id": "xsrHFQKP0JSC"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### C3d"
      ],
      "metadata": {
        "id": "qLrrYg_j0Nbf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "o-GkQRvnVpMa"
      },
      "outputs": [],
      "source": [
        "def run_demo_c3d(sample_video_path, video_name):\n",
        "    # read video\n",
        "    video_clips, num_frames = get_video_clips(sample_video_path, c3d_frame_count)\n",
        "\n",
        "    print(\"Number of clips in the video : \", len(video_clips))\n",
        "\n",
        "    # build models\n",
        "    feature_extractor = c3d_feature_extractor()\n",
        "    classifier_model = model_c3d\n",
        "\n",
        "    print(\"Models initialized\")\n",
        "\n",
        "    # extract features\n",
        "    rgb_features = []\n",
        "    for i, clip in enumerate(video_clips):\n",
        "        clip = np.array(clip)\n",
        "        if len(clip) < c3d_frame_count:\n",
        "            continue\n",
        "\n",
        "        clip = preprocess_input_c3d(clip)\n",
        "        rgb_feature = feature_extractor.predict(clip)[0]\n",
        "        rgb_features.append(rgb_feature)\n",
        "\n",
        "        print(\"Processed clip : \", i)\n",
        "\n",
        "    rgb_features = np.array(rgb_features)\n",
        "    print(rgb_features.shape)\n",
        "    rgb_feature_bag = interpolate(rgb_features, features_per_bag)\n",
        "    \n",
        "    # classify using the trained classifier model\n",
        "    predictions = classifier_model.predict(rgb_feature_bag)\n",
        "\n",
        "    predictions = np.array(predictions).squeeze()\n",
        "\n",
        "    predictions = extrapolate(predictions, num_frames)\n",
        "    \n",
        "    save_path = os.path.join(output_folder_c3d, video_name + '_10.gif')\n",
        "    # visualize predictions\n",
        "    print('Executed Successfully - '+video_name + '.gif saved')\n",
        "    visualize_predictions(sample_video_path, predictions, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_9897hBWR0W"
      },
      "outputs": [],
      "source": [
        "run_demo_c3d('/content/Shooting050_x264.mp4', 'Shooting050_x264')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# c3d testing"
      ],
      "metadata": {
        "id": "HigKm-7_UbOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_demo_c3d('/content/Shooting050_x264.mp4', 'Shooting050_x264')"
      ],
      "metadata": {
        "id": "Qt68Ama5UaOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# i3d testing"
      ],
      "metadata": {
        "id": "QeTBZXbDYBcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_demo_i3d('/content/Shooting050_x264.mp4', 'Shooting050_x264')"
      ],
      "metadata": {
        "id": "WYIIPpnTUePF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "91Whhz7wYjv4",
        "B0zyF-v_CZCg",
        "LjY1bDQCbxMr",
        "AhPxsMW0yQUu",
        "lPApAj59yUXp",
        "S3jDISdJg5Ln",
        "lKzL2lYHioRU",
        "dvv1W7mPisHb",
        "P6Ij0gMT2V3B",
        "5iQX04Gm5K4h",
        "Dbw458e65bDv",
        "PDNVo47z5fMk",
        "PfNhzIxo5hXA",
        "Hv0MOr8F5jPo",
        "T2s89ou05m3o",
        "bjsZmv-15rS9",
        "LKJT0Uok5uaO",
        "PFbc5TLGbV2N",
        "b-UOUFkiVkBj"
      ],
      "name": "Anomaly Detection.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}